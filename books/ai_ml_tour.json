[
  {
    "n": 1,
    "title": "Module 1: Introduction - What is AI, ML, and Deep Learning?",
    "math": "N/A",
    "note": "Define and differentiate between Artificial Intelligence, Machine Learning, and Deep Learning. Provide a high-level overview of the course structure, emphasizing the progression from fundamentals to state-of-the-art models."
  },
  {
    "n": 2,
    "title": "Setting up Your Environment: Google Colab and PyTorch",
    "math": "N/A",
    "note": "Introduce Google Colab as a browser-based Python environment with free access to GPUs. Guide through creating a new notebook, basic navigation, and enabling GPU acceleration. Confirm PyTorch is installed."
  },
  {
    "n": 3,
    "title": "Module 2: Core Libraries - Intro to NumPy",
    "math": "N/A",
    "note": "Introduce NumPy as the fundamental package for numerical computing in Python. Explain the `ndarray` object."
  },
  {
    "n": 4,
    "title": "NumPy: Array Creation and Indexing",
    "math": "N/A",
    "note": "Show how to create NumPy arrays from lists, using `np.zeros`, `np.ones`, and `np.arange`. Cover basic indexing and slicing."
  },
  {
    "n": 5,
    "title": "NumPy: Array Math and Broadcasting",
    "math": "Linear Algebra",
    "note": "Teach element-wise operations and the concept of broadcasting for performing math on arrays of different shapes."
  },
  {
    "n": 6,
    "title": "Intro to Pandas",
    "math": "N/A",
    "note": "Introduce pandas as the primary library for data manipulation and analysis. Explain the `Series` and `DataFrame` objects."
  },
  {
    "n": 7,
    "title": "Pandas: Loading Data from CSV",
    "math": "N/A",
    "note": "Show how to load a dataset from a CSV file into a pandas DataFrame using `pd.read_csv()`."
  },
  {
    "n": 8,
    "title": "Pandas: Data Inspection",
    "math": "Statistics",
    "note": "Teach how to inspect a DataFrame using `.head()`, `.tail()`, `.info()`, and `.describe()`."
  },
  {
    "n": 9,
    "title": "Pandas: Selection and Filtering",
    "math": "Boolean Algebra",
    "note": "Cover how to select columns and rows using `[]`, `.loc`, and `.iloc`. Show how to filter data based on conditions."
  },
  {
    "n": 10,
    "title": "Intro to Matplotlib and Seaborn",
    "math": "N/A",
    "note": "Introduce Matplotlib as the foundational plotting library and Seaborn as a high-level interface for creating attractive statistical plots."
  },
  {
    "n": 11,
    "title": "Creating Basic Plots: Histograms and Scatter Plots",
    "math": "Statistics",
    "note": "Show how to create a histogram to view data distribution and a scatter plot to investigate relationships between two variables."
  },
  {
    "n": 12,
    "title": "Intro to PyTorch",
    "math": "N/A",
    "note": "Introduce PyTorch as a powerful deep learning framework. Explain the concept of Tensors."
  },
  {
    "n": 13,
    "title": "PyTorch: Creating Tensors",
    "math": "N/A",
    "note": "Show how to create PyTorch tensors and perform basic operations. Explain the importance of `torch.device` for moving tensors to the GPU."
  },
  {
    "n": 14,
    "title": "Module 3: Mathematical Foundations - Vectors and Matrices",
    "math": "Linear Algebra",
    "note": "Cover vectors, matrices, dot products, and matrix multiplication. Implement these operations using PyTorch tensors."
  },
  {
    "n": 15,
    "title": "Derivatives and Gradients",
    "math": "Calculus",
    "note": "Introduce derivatives as a measure of change and the gradient as a vector of partial derivatives. Explain their role in optimization."
  },
  {
    "n": 16,
    "title": "The Chain Rule and Autograd",
    "math": "Calculus",
    "note": "Explain the chain rule for differentiating composite functions. Introduce PyTorch's `autograd` engine and the `.backward()` method for automatic gradient computation."
  },
  {
    "n": 17,
    "title": "Probability and Statistics Fundamentals",
    "math": "Probability, Statistics",
    "note": "Cover basic probability theory, random variables, and key statistical measures like mean, variance, and standard deviation."
  },
  {
    "n": 18,
    "title": "Module 4: Machine Learning Fundamentals - Data Preprocessing",
    "math": "N/A",
    "note": "Cover essential data preprocessing steps: handling missing data, encoding categorical variables (One-Hot), and feature scaling (Standardization)."
  },
  {
    "n": 19,
    "title": "Training, Validation, and Test Sets",
    "math": "N/A",
    "note": "Explain the importance of splitting data to evaluate model generalization. Use scikit-learn's `train_test_split`."
  },
  {
    "n": 20,
    "title": "Linear Regression",
    "math": "Linear Algebra",
    "note": "Introduce linear regression. Implement a simple model from scratch using PyTorch, defining the model, MSE loss, and an optimizer."
  },
  {
    "n": 21,
    "title": "Gradient Descent",
    "math": "Calculus",
    "note": "Explain the gradient descent algorithm. Manually implement a training loop to update model weights based on calculated gradients."
  },
  {
    "n": 22,
    "title": "Logistic Regression for Classification",
    "math": "Probability",
    "note": "Introduce logistic regression for binary classification. Explain the sigmoid function and binary cross-entropy loss."
  },
  {
    "n": 23,
    "title": "Overfitting and Regularization",
    "math": "N/A",
    "note": "Explain overfitting. Introduce L1 and L2 regularization as techniques to penalize large weights and improve model generalization."
  },
  {
    "n": 24,
    "title": "Module 5: Deep Learning - Neural Networks",
    "math": "N/A",
    "note": "Introduce the feedforward neural network (MLP). Explain layers, neurons, weights, biases, and activation functions (ReLU)."
  },
  {
    "n": 25,
    "title": "Building an MLP with PyTorch",
    "math": "N/A",
    "note": "Show how to define a neural network in PyTorch using `nn.Module` and `nn.Linear` layers."
  },
  {
    "n": 26,
    "title": "Training a Neural Network",
    "math": "N/A",
    "note": "Write a complete training loop for a PyTorch neural network, including the forward pass, loss calculation, backpropagation, and weight updates."
  },
  {
    "n": 27,
    "title": "Convolutional Neural Networks (CNNs)",
    "math": "Linear Algebra",
    "note": "Introduce CNNs for image data. Explain the convolution operation (`nn.Conv2d`) and pooling (`nn.MaxPool2d`)."
  },
  {
    "n": 28,
    "title": "Building a CNN for Image Classification",
    "math": "N/A",
    "note": "Build and train a CNN on a dataset like MNIST or CIFAR-10 using PyTorch, including data loading with `torchvision`."
  },
  {
    "n": 29,
    "title": "Module 6: Transformers - The Attention Mechanism",
    "math": "Linear Algebra",
    "note": "Introduce the concept of attention as a mechanism to weigh the importance of different parts of an input sequence. Explain Query, Key, and Value vectors."
  },
  {
    "n": 30,
    "title": "Scaled Dot-Product Attention",
    "math": "N/A",
    "note": "Implement the scaled dot-product attention formula from scratch using PyTorch."
  },
  {
    "n": 31,
    "title": "Multi-Head Attention",
    "math": "N/A",
    "note": "Explain how multi-head attention allows the model to jointly attend to information from different representation subspaces. Implement a `MultiHeadAttention` module."
  },
  {
    "n": 32,
    "title": "Positional Encodings",
    "math": "Trigonometry",
    "note": "Explain why positional encodings are necessary for transformers to understand sequence order. Implement sinusoidal positional encodings."
  },
  {
    "n": 33,
    "title": "The Transformer Encoder Block",
    "math": "N/A",
    "note": "Build a complete Transformer encoder block, combining multi-head attention with feed-forward layers, residual connections, and layer normalization."
  },
  {
    "n": 34,
    "title": "The Transformer Decoder Block",
    "math": "N/A",
    "note": "Build a Transformer decoder block, explaining the masked multi-head attention mechanism used to prevent attending to future tokens."
  },
  {
    "n": 35,
    "title": "Module 7: Generative Models - GANs",
    "math": "Game Theory",
    "note": "Introduce Generative Adversarial Networks (GANs). Explain the two-player game between the Generator and the Discriminator."
  },
  {
    "n": 36,
    "title": "StyleGAN",
    "math": "N/A",
    "note": "Introduce the StyleGAN architecture. Focus on the Mapping Network that transforms latent codes into a style vector `w` and the Synthesis Network that uses AdaIN to control the image features."
  },
  {
    "n": 37,
    "title": "Diffusion Models",
    "math": "N/A",
    "note": "Introduce Diffusion Models. Explain the forward process (gradually adding noise) and the reverse process (learning to denoise)."
  },
  {
    "n": 38,
    "title": "Module 8: Multimodal AI - The Concept",
    "math": "N/A",
    "note": "Introduce multimodal AI as models that can process and relate information from multiple data types, such as images and text."
  },
  {
    "n": 39,
    "title": "CLIP: The Architecture",
    "math": "N/A",
    "note": "Explain the architecture of OpenAI's CLIP model: an image encoder and a text encoder that are trained to map images and their captions to the same point in a latent space."
  },
  {
    "n": 40,
    "title": "CLIP: Contrastive Loss",
    "math": "Linear Algebra (Cosine Similarity)",
    "note": "Explain the contrastive learning objective used to train CLIP, which maximizes the similarity of correct image-text pairs and minimizes the similarity of incorrect pairs."
  },
  {
    "n": 41,
    "title": "Module 9: Final Projects - Overview",
    "math": "N/A",
    "note": "Outline the three advanced final projects: an image generation model, a chat model, and a multimodal search model."
  },
  {
    "n": 42,
    "title": "Final Project 1: Image Generation with Diffusion Models",
    "math": "N/A",
    "note": "Guide through training a simple diffusion model on a dataset like Fashion-MNIST to generate new clothing items."
  },
  {
    "n": 43,
    "title": "Final Project 2: Building a Transformer Chat Model",
    "math": "N/A",
    "note": "Guide through building and training a character-level transformer model from scratch on a small text corpus to generate new text."
  },
  {
    "n": 44,
    "title": "Final Project 3: Image Search with a CLIP-like Model",
    "math": "N/A",
    "note": "Guide through training a simplified CLIP model on a dataset of image-caption pairs to enable natural language image search."
  },
  {
    "n": 45,
    "title": "Module 10: Further Reading",
    "math": "N/A",
    "note": "Provide links to key papers ('Attention Is All You Need', 'Denoising Diffusion Probabilistic Models'), popular libraries (Hugging Face), and advanced deep learning courses."
  }
]
