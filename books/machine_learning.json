[
  {
    "n": 1,
    "title": "What Is Machine Learning?",
    "math": "Statistical learning",
    "note": "This section provides a formal definition of Machine Learning as a field of AI that uses statistical techniques to give computer systems the ability to 'learn' from data, without being explicitly programmed."
  },
  {
    "n": 2,
    "title": "Types of Machine Learning",
    "math": "Learning paradigms",
    "note": "This lesson categorizes the main types of machine learning. It covers Supervised Learning (learning from labeled data), Unsupervised Learning (finding patterns in unlabeled data), and Reinforcement Learning (learning through trial and error)."
  },
  {
    "n": 3,
    "title": "The Machine Learning Workflow",
    "math": "Process model",
    "note": "This section outlines the typical end-to-end process of a machine learning project: defining the problem, data preparation, choosing a model, training, evaluation, hyperparameter tuning, and deployment."
  },
  {
    "n": 4,
    "title": "Supervised Learning: Regression",
    "math": "Regression analysis",
    "note": "This lesson introduces Regression as a supervised learning task where the goal is to predict a continuous numerical value. The core algorithm, Linear Regression, is introduced, explaining how it models a linear relationship between input features and a target variable."
  },
  {
    "n": 5,
    "title": "Training a Model: Cost Functions and Gradient Descent",
    "math": "Gradient descent",
    "note": "This lesson explains how a model is trained. It introduces a Cost Function (like Mean Squared Error) to measure model error, and explains Gradient Descent as an optimization algorithm that iteratively adjusts the model's parameters to minimize this cost."
  },
  {
    "n": 6,
    "title": "Supervised Learning: Classification",
    "math": "Classification analysis",
    "note": "This lesson introduces Classification as a supervised learning task where the goal is to predict a discrete class label (e.g., 'spam' or 'not spam')."
  },
  {
    "n": 7,
    "title": "Classification: Logistic Regression",
    "math": "Logistic regression",
    "note": "This section covers Logistic Regression, a core algorithm for binary classification. It uses the logistic (sigmoid) function to squash the output of a linear equation to a value between 0 and 1, representing a probability."
  },
  {
    "n": 8,
    "title": "Classification: Decision Trees",
    "math": "Decision trees",
    "note": "This lesson introduces Decision Trees, an intuitive model that learns to classify data by creating a tree of if-then-else rules. It explains how the tree is built by finding the best features to split the data on at each node."
  },
  {
    "n": 9,
    "title": "Classification: Support Vector Machines (SVMs)",
    "math": "Support Vector Machines",
    "note": "This lesson covers SVMs, a powerful classification algorithm. It explains the core concept of finding the optimal hyperplane that maximally separates the classes. It also introduces the 'kernel trick,' which allows SVMs to perform non-linear classification."
  },
  {
    "n": 10,
    "title": "The Bias-Variance Tradeoff",
    "math": "Bias-variance tradeoff",
    "note": "This crucial lesson explains a fundamental concept. Bias is the error from erroneous assumptions (underfitting). Variance is the error from sensitivity to small fluctuations in the training data (overfitting). The goal is to find a balance between the two."
  },
  {
    "n": 11,
    "title": "Overfitting and Regularization",
    "math": "Regularization",
    "note": "This section focuses on overfitting, where a model fails to generalize to new data. It introduces Regularization as a technique to prevent overfitting by adding a penalty to the cost function for large model coefficients. L1 (Lasso) and L2 (Ridge) regularization are explained."
  },
  {
    "n": 12,
    "title": "Model Evaluation for Classification",
    "math": "Precision and recall",
    "note": "This lesson details how to evaluate a classification model. It explains the Confusion Matrix and the key metrics derived from it: Accuracy, Precision (the accuracy of positive predictions), and Recall (the ability to find all positive samples)."
  },
  {
    "n": 13,
    "title": "Model Validation with Cross-Validation",
    "math": "Resampling methods",
    "note": "An essential lesson on robust model evaluation. It introduces K-Fold Cross-Validation as a technique to get a reliable estimate of a model's performance on unseen data by training and testing on different subsets of the data, which helps to prevent overfitting."
  },
  {
    "n": 14,
    "title": "Hyperparameter Tuning",
    "math": "Optimization",
    "note": "A practical lesson on tuning a model. It explains the difference between model parameters (learned from data) and hyperparameters (set by the developer) and introduces techniques like Grid Search and Random Search to find the optimal hyperparameter settings."
  },
  {
    "n": 15,
    "title": "Ensemble Learning: Random Forests",
    "math": "Ensemble learning (Random Forests)",
    "note": "This lesson introduces Ensemble Learning. It focuses on Random Forests, an ensemble of many decision trees. It explains how Random Forests reduce overfitting by introducing randomness in both data sampling (bagging) and feature selection."
  },
  {
    "n": 16,
    "title": "Ensemble Learning: Gradient Boosting",
    "math": "Boosting algorithms",
    "note": "This section covers another type of ensemble learning called Boosting. It explains how boosting works by building models sequentially, where each new model attempts to correct the errors of the previous one. Gradient Boosting is presented as a powerful and widely used algorithm."
  },
  {
    "n": 17,
    "title": "Unsupervised Learning: Clustering with K-Means",
    "math": "k-Means clustering",
    "note": "This lesson introduces Unsupervised Learning, which finds patterns in unlabeled data. It focuses on Clustering and details the k-Means algorithm, an iterative method for partitioning data into 'k' distinct clusters."
  },
  {
    "n": 18,
    "title": "Unsupervised Learning: Dimensionality Reduction with PCA",
    "math": "Principal Component Analysis (PCA)",
    "note": "This lesson explains Dimensionality Reduction. It introduces PCA as the most common technique, which works by identifying the directions (principal components) that capture the maximum variance in the data and projecting the data onto a new, lower-dimensional subspace."
  },
  {
    "n": 19,
    "title": "Introduction to Deep Learning",
    "math": "Artificial Neural Networks",
    "note": "This lesson provides a brief introduction to Deep Learning as a subfield of ML based on Artificial Neural Networks with many layers. It explains how these networks can automatically learn complex representations from data, leading to breakthroughs in fields like computer vision and NLP."
  },
  {
    "n": 20,
    "title": "Introduction to Reinforcement Learning",
    "math": "Markov decision processes",
    "note": "A high-level conceptual lesson that completes the overview of the three main ML paradigms. It explains the core components of Reinforcement Learning: the agent, the environment, and the process of learning through actions and rewards to maximize a cumulative reward."
  }
]
