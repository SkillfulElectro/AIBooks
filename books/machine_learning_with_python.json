[
  {
    "n": 1,
    "title": "Why Python for Machine Learning?",
    "math": "Programming language ecosystems",
    "note": "This section explains why Python is the dominant language for machine learning, highlighting its simple syntax, extensive libraries (Scikit-learn, Pandas, TensorFlow), and strong community support. It recommends setting up with the Anaconda distribution and Jupyter Notebooks."
  },
  {
    "n": 2,
    "title": "Essential Library: NumPy",
    "math": "N-dimensional arrays",
    "note": "This section introduces NumPy and its core data structure, the N-dimensional array. It explains how NumPy enables efficient vectorized operations, which are much faster than standard Python lists for numerical computing."
  },
  {
    "n": 3,
    "title": "Essential Library: Pandas",
    "math": "Data frames",
    "note": "This section introduces Pandas for data manipulation. It explains the primary data structure, the `DataFrame`, and demonstrates how to read data from a CSV file (`pd.read_csv()`) and inspect it with `.head()`, `.info()`, and `.describe()`."
  },
  {
    "n": 4,
    "title": "Pandas: Selection and Filtering",
    "math": "Data selection",
    "note": "This section covers how to select specific rows and columns from a DataFrame. It explains how to select columns by name and how to use boolean indexing to filter rows based on conditions. The `.loc` and `.iloc` accessors are introduced."
  },
  {
    "n": 5,
    "title": "Essential Library: Matplotlib and Seaborn",
    "math": "Data visualization",
    "note": "This lesson introduces Matplotlib as the foundational plotting library and Seaborn as a high-level interface for creating attractive statistical plots. The importance of visualizing data to find patterns and outliers is emphasized."
  },
  {
    "n": 6,
    "title": "Introduction to Scikit-Learn",
    "math": "Machine learning APIs",
    "note": "This section introduces Scikit-learn as the core ML library in Python. It highlights its consistent 'Estimator' API, demonstrating the standard workflow: import a class, instantiate it, train it with `.fit()`, and make predictions with `.predict()`."
  },
  {
    "n": 7,
    "title": "Data Preparation: `train_test_split`",
    "math": "Data splitting",
    "note": "This lesson explains a crucial step in model evaluation. It demonstrates how to use the `train_test_split` function from Scikit-learn to split a dataset into a training set and a testing set to evaluate the model's performance on unseen data."
  },
  {
    "n": 8,
    "title": "Data Preparation: Feature Scaling",
    "math": "Data normalization and standardization",
    "note": "This section explains why feature scaling is a critical pre-processing step for many ML algorithms. It introduces the `StandardScaler` (for standardization) and the `MinMaxScaler` (for normalization) from Scikit-learn."
  },
  {
    "n": 9,
    "title": "Data Preparation: Handling Categorical Data",
    "math": "Categorical data encoding",
    "note": "A more focused lesson on this common pre-processing challenge. It demonstrates how to use Scikit-learn's `OneHotEncoder` to convert categorical features into a numerical format suitable for machine learning models."
  },
  {
    "n": 10,
    "title": "Supervised Learning: Linear and Logistic Regression",
    "math": "Linear models implementation",
    "note": "This lesson provides a practical example of building basic models. It shows how to import, train, and use the `LinearRegression` class for regression tasks and the `LogisticRegression` class for classification tasks."
  },
  {
    "n": 11,
    "title": "Supervised Learning: K-Nearest Neighbors (k-NN)",
    "math": "k-NN implementation",
    "note": "This lesson covers the implementation of the k-NN classification algorithm. It shows how to use the `KNeighborsClassifier` from Scikit-learn and reiterates the importance of feature scaling for this distance-based algorithm."
  },
  {
    "n": 12,
    "title": "Supervised Learning: Decision Trees and Random Forests",
    "math": "Tree-based model implementation",
    "note": "This lesson shows how to implement tree-based models. It covers using the `DecisionTreeClassifier` for a single tree and the `RandomForestClassifier` for an ensemble of trees, demonstrating the improvement in performance."
  },
  {
    "n": 13,
    "title": "Supervised Learning: SVMs and Gradient Boosting",
    "math": "SVM and Boosting implementation",
    "note": "A lesson that adds two more powerful algorithms. It demonstrates how to implement `SVC` (Support Vector Classifier) and a gradient boosting classifier (like `XGBClassifier`) using their Scikit-learn compatible APIs."
  },
  {
    "n": 14,
    "title": "Unsupervised Learning: K-Means Clustering",
    "math": "k-Means implementation",
    "note": "This section covers the implementation of the most popular unsupervised learning algorithm. It demonstrates how to use the `KMeans` class from Scikit-learn to group unlabeled data into a specified number of clusters."
  },
  {
    "n": 15,
    "title": "Hyperparameter Tuning with `GridSearchCV`",
    "math": "Hyperparameter optimization",
    "note": "This lesson explains how to tune hyperparameters (parameters set before training). It introduces `GridSearchCV` from Scikit-learn as a tool for automatically searching for the best combination of hyperparameters for a model."
  },
  {
    "n": 16,
    "title": "Building Pipelines",
    "math": "Workflow automation",
    "note": "This section introduces Scikit-learn's `Pipeline` object. It demonstrates how to create a pipeline that chains together multiple steps, such as feature scaling and model training, into a single object. This helps to prevent data leakage and simplifies code."
  },
  {
    "n": 17,
    "title": "Saving and Loading Models",
    "math": "Model persistence",
    "note": "A crucial practical lesson on model persistence. It explains how to save a trained Scikit-learn model to a file using `joblib` and then load it back later for making predictions, which is essential for deploying models into applications."
  },
  {
    "n": 18,
    "title": "Dealing with Imbalanced Data",
    "math": "Resampling techniques",
    "note": "A lesson on the problem of imbalanced datasets. It explains why accuracy is a poor metric in this case and introduces the strategy of over-sampling the minority class using the SMOTE algorithm from the `imbalanced-learn` library."
  },
  {
    "n": 19,
    "title": "Application: Sentiment Analysis with `TfidfVectorizer`",
    "math": "Text feature extraction",
    "note": "This lesson ties together many concepts by building a simple sentiment analysis model. It introduces `TfidfVectorizer` to convert text data into numerical features and then trains a classifier to predict sentiment."
  },
  {
    "n": 20,
    "title": "Introduction to Deep Learning with Keras/TensorFlow",
    "math": "Neural network implementation",
    "note": "This final lesson provides a brief, practical introduction to deep learning in Python. It introduces the Keras API and shows how to build, compile, fit, and evaluate a simple sequential neural network for a classification task."
  }
]
