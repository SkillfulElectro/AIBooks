[
  {
    "n": 1,
    "title": "What Is Statistics?",
    "math": "Statistics",
    "note": "Defines Statistics as the science of collecting, analyzing, interpreting, and organizing data. Explains the two main branches: Descriptive Statistics (summarizing data) and Inferential Statistics (drawing conclusions from data)."
  },
  {
    "n": 2,
    "title": "Populations, Samples, and Experimental Design",
    "math": "Sampling theory",
    "note": "Introduces populations vs. samples. Covers sampling methods (simple random, stratified) and principles of experimental design, including control groups, randomization, and blinding, to ensure valid inferences."
  },
  {
    "n": 3,
    "title": "Descriptive Statistics: Central Tendency and Spread",
    "math": "Mean, median, mode, variance, standard deviation",
    "note": "Covers describing a dataset's center (Mean, Median, Mode) and its variability or dispersion (Range, Variance, Standard Deviation)."
  },
  {
    "n": 4,
    "title": "Visualizing Data: Histograms and Box Plots",
    "math": "Histograms, quartiles, box plots",
    "note": "Covers graphical representations of data distributions. Explains histograms for frequency and box plots for visualizing the five-number summary (minimum, Q1, median, Q3, maximum)."
  },
  {
    "n": 5,
    "title": "Introduction to Probability",
    "math": "Probability theory",
    "note": "Introduces probability as a measure of the likelihood of an event, from 0 (impossible) to 1 (certain). Defines experiment, sample space, and event."
  },
  {
    "n": 6,
    "title": "Conditional Probability and Bayes' Theorem",
    "math": "Conditional probability, Bayes' theorem",
    "note": "Covers the probability of an event occurring given another has occurred. Introduces Bayes' Theorem for relating conditional probabilities, a foundation of Naive Bayes classifiers."
  },
  {
    "n": 7,
    "title": "Probability Distributions (Normal Distribution)",
    "math": "Normal distribution",
    "note": "Introduces probability distributions, focusing on the Normal (Gaussian) distribution. Explains its symmetric, bell-shaped curve and its importance in statistics."
  },
  {
    "n": 8,
    "title": "The Central Limit Theorem",
    "math": "Central Limit Theorem (CLT)",
    "note": "Covers the CLT, which states that the sampling distribution of the mean will be approximately normal for large samples, regardless of the population's distribution. This is the foundation of inferential statistics."
  },
  {
    "n": 9,
    "title": "Confidence Intervals",
    "math": "Confidence intervals",
    "note": "Introduces Confidence Intervals as an estimated range of values which is likely to include an unknown population parameter (like the mean), based on a certain confidence level (e.g., 95%)."
  },
  {
    "n": 10,
    "title": "Introduction to Hypothesis Testing",
    "math": "Hypothesis testing",
    "note": "Introduces the formal procedure for using sample data to evaluate a claim. Explains setting up a Null Hypothesis (H0, the default) and an Alternative Hypothesis (H1, the claim to test)."
  },
  {
    "n": 11,
    "title": "Understanding the p-value",
    "math": "p-value",
    "note": "Defines the p-value: the probability of observing results as extreme as those found, assuming the null hypothesis is true. A small p-value (typically â‰¤ 0.05) is evidence against the null hypothesis."
  },
  {
    "n": 12,
    "title": "Type I & Type II Errors and Statistical Power",
    "math": "Type I/II errors, Statistical Power",
    "note": "Explains errors in hypothesis testing: Type I (false positive) and Type II (false negative). Introduces Statistical Power as the probability of correctly rejecting a false null hypothesis (avoiding a Type II error)."
  },
  {
    "n": 13,
    "title": "Hypothesis Test: The t-test",
    "math": "Student's t-test",
    "note": "Introduces the t-test, used to determine if there is a significant difference between the means of two groups, especially with small sample sizes."
  },
  {
    "n": 14,
    "title": "Hypothesis Test: ANOVA (Analysis of Variance)",
    "math": "ANOVA",
    "note": "Introduces ANOVA, a statistical test used to compare the means of three or more groups to see if at least one group is different from the others."
  },
  {
    "n": 15,
    "title": "Hypothesis Test: The Chi-Squared Test",
    "math": "Chi-squared test",
    "note": "Introduces the Chi-Squared test, used for analyzing categorical data. It assesses the goodness of fit or tests the independence of two categorical variables."
  },
  {
    "n": 16,
    "title": "Introduction to Non-parametric Tests",
    "math": "Non-parametric statistics",
    "note": "Introduces non-parametric tests (e.g., Wilcoxon rank-sum, Kruskal-Wallis) which do not assume the data follows a specific distribution. Useful for ordinal data or when assumptions for parametric tests are not met."
  },
  {
    "n": 17,
    "title": "Correlation and Causation",
    "math": "Correlation vs. causation",
    "note": "Explains correlation as a measure of linear relationship (from -1 to 1). Emphasizes the critical principle that correlation does not imply causation, often due to confounding variables."
  },
  {
    "n": 18,
    "title": "Simple Linear Regression",
    "math": "Simple linear regression",
    "note": "Introduces Simple Linear Regression to model the relationship between two continuous variables by fitting a best-fit line. Defines concepts like slope and intercept."
  },
  {
    "n": 19,
    "title": "Multiple Linear Regression",
    "math": "Multiple linear regression",
    "note": "Expands to Multiple Linear Regression, using several independent variables to predict a single continuous dependent variable. Introduces model evaluation concepts like R-squared."
  },
  {
    "n": 20,
    "title": "Logistic Regression",
    "math": "Logistic regression",
    "note": "Introduces Logistic Regression, the fundamental method for classification problems. Explains how it's used to predict a binary (categorical) outcome based on one or more predictor variables."
  }
]
